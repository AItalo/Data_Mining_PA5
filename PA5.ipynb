{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA5 - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Train Dataset\n",
    "\n",
    "For this step, we will create a Naive Bayes classifier for the \"train\" dataset, provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    [\"weekday\", \"spring\", \"none\", \"none\", \"on time\"],\n",
    "    [\"weekday\", \"winter\", \"none\", \"slight\", \"on time\"],\n",
    "    [\"weekday\", \"winter\", \"none\", \"slight\", \"on time\"],\n",
    "    [\"weekday\", \"winter\", \"high\", \"heavy\", \"late\"], \n",
    "    [\"saturday\", \"summer\", \"normal\", \"none\", \"on time\"],\n",
    "    [\"weekday\", \"autumn\", \"normal\", \"none\", \"very late\"],\n",
    "    [\"holiday\", \"summer\", \"high\", \"slight\", \"on time\"],\n",
    "    [\"sunday\", \"summer\", \"normal\", \"none\", \"on time\"],\n",
    "    [\"weekday\", \"winter\", \"high\", \"heavy\", \"very late\"],\n",
    "    [\"weekday\", \"summer\", \"none\", \"slight\", \"on time\"],\n",
    "    [\"saturday\", \"spring\", \"high\", \"heavy\", \"cancelled\"],\n",
    "    [\"weekday\", \"summer\", \"high\", \"slight\", \"on time\"],\n",
    "    [\"saturday\", \"winter\", \"normal\", \"none\", \"late\"],\n",
    "    [\"weekday\", \"summer\", \"high\", \"none\", \"on time\"],\n",
    "    [\"weekday\", \"winter\", \"normal\", \"heavy\", \"very late\"],\n",
    "    [\"saturday\", \"autumn\", \"high\", \"slight\", \"on time\"],\n",
    "    [\"weekday\", \"autumn\", \"none\", \"heavy\", \"on time\"],\n",
    "    [\"holiday\", \"spring\", \"normal\", \"slight\", \"on time\"],\n",
    "    [\"weekday\", \"spring\", \"normal\", \"none\", \"on time\"],\n",
    "    [\"weekday\", \"spring\", \"normal\", \"slight\", \"on time\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create this classifier, we first must calculate prior probabilities for each class label.\n",
    "We will do this with the following helper function:\n",
    "\n",
    "* `calculate_priors()`\n",
    "    * **Params**:\n",
    "        * `data` - The dataset to calculate prior probabilities for\n",
    "        * `index` - The index of the classifier value in the dataset\n",
    "        * `classes` - An array of the classes present in the dataset\n",
    "    * **Returns**:\n",
    "        * An array of prior probability values, ordered according to the `classes` param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_priors(data, index, classes):\n",
    "    counts = {}\n",
    "    total = 0\n",
    "    for label in classes:\n",
    "        counts[label] = 0\n",
    "    for instance in data:\n",
    "        counts[instance[index]] += 1\n",
    "        total += 1\n",
    "    probabilities = []\n",
    "    for label in classes:\n",
    "        probabilities.append(counts[label] / total)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check these values, we will refer to **Figure 3.2** from the Bramer textbook, which claims that the Prior Probabilities for \"on time\", \"late\" \"very late\", and \"cancelled\" should be 0.70, 0.10, 0.15, and 0.05, respectively.\n",
    "\n",
    "We will run our `calculatePriors()` function on these labels and display the values, which should match the given probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7, 0.1, 0.15, 0.05]\n"
     ]
    }
   ],
   "source": [
    "priors = calculate_priors(table, 4, [\"on time\", \"late\", \"very late\", \"cancelled\"])\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values returned from `calculate_priors()` are the same as given in Bramer. Next, we will calculate the posterior probabilities for a given class label. Again, we will create a helper function `calculate_posteriors()` to find these values:\n",
    "\n",
    "* `calculate_posteriors()`\n",
    "    * **Params**:\n",
    "        * `data` - The dataset to calculate probabilities for\n",
    "        * `attributeIndex` - The index of the attribute to calculate conditional probability for\n",
    "        * `attribute` - The value of the index to calculate conditional probability for\n",
    "        * `classIndex` - The index of the class label\n",
    "        * `classLabels` - All classifier labels to calculate conditional probabilities for\n",
    "    * **Returns**:\n",
    "        * A list of posterior probabilities for the given attribute over each class label, ordered with respect to `classLabels()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posteriors(data, attributeIndex, attribute, classIndex, classLabels):\n",
    "    conditionalCounts = {}\n",
    "    counts = {}\n",
    "    for label in classLabels:\n",
    "        counts[label] = 0\n",
    "        conditionalCounts[label] = 0\n",
    "    for instance in data:\n",
    "        counts[instance[classIndex]] += 1\n",
    "        if instance[attributeIndex] == attribute:\n",
    "            conditionalCounts[instance[classIndex]] += 1\n",
    "    probabilities = []\n",
    "    for label in classLabels:\n",
    "        if counts[label] == 0:\n",
    "            probabilities.append(0)\n",
    "        else:\n",
    "            probabilities.append(conditionalCounts[label] / counts[label])\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we will check our function output against the values provided by Bramer. For the attribute (day = \"weekday\"), we expect the posterior probabilities for class = \"on time\", \"late\", \"very late\", and \"cancelled\" to be 0.64, 0.5, 1, and 0, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6428571428571429, 0.5, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "posteriors = calculate_posteriors(table, 0, \"weekday\", 4, [\"on time\", \"late\", \"very late\", \"cancelled\"])\n",
    "print(posteriors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our values match up, although Bramer's values round off to 2 decimal places while ours sometimes have more bits of precision.\n",
    "\n",
    "Finally, we can use this to create a Naive Bayes classifier function.\n",
    "\n",
    "* `naive_bayes_classify()`\n",
    "    * **Params**:\n",
    "        * `train` - The data to use as training data\n",
    "        * `classIndex` - The index of the class label\n",
    "        * `classLabels` - A list of all possible class labels\n",
    "        * `test` - The unseen data to classify\n",
    "    * **Returns**:\n",
    "        * A classification for the `test` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classify(train, classIndex, classLabels, test):\n",
    "    classProbabilities = []\n",
    "    for val in classLabels:\n",
    "        classProbabilities.append(0)\n",
    "    priors = calculate_priors(train, classIndex, classLabels)\n",
    "    for i in range(len(classProbabilities)):\n",
    "        classProbabilities[i] += priors[i]\n",
    "    for i in range(len(test)):\n",
    "        if i == classIndex:\n",
    "            continue\n",
    "        else:\n",
    "            attribute = test[i]\n",
    "            posteriors = calculate_posteriors(train, i, attribute, classIndex, classLabels)\n",
    "            for i in range(len(posteriors)):\n",
    "                classProbabilities[i] *= posteriors[i]\n",
    "    maxP = 0\n",
    "    index = 0\n",
    "    for i in range(len(classProbabilities)):\n",
    "        if classProbabilities[i] > maxP:\n",
    "            maxP = classProbabilities[i]\n",
    "            index = i\n",
    "    return classLabels[index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that our classifier is functioning as intended, we will test it on the trains dataset using the unseen value (\"weekday\", \"winter\", \"high\", \"heavy\", \"???\"). Accordign to Bramer, this should be classified as \"very late\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very late\n"
     ]
    }
   ],
   "source": [
    "label = naive_bayes_classify(table, 4, [\"on time\", \"late\", \"very late\", \"cancelled\"], [\"weekday\", \"winter\", \"high\", \"heavy\", \"???\"])\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as shown, the `naive_bayes_classify()` function does indeed classify the unseen data correctly. Therefore, we now have a functioning method for using Naive Bayes classification accross a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MPG predictor\n",
    "\n",
    "For this step, we will use our Naive Bayes methods on the auo-data dataset. First, we will import the data into an array titled `auto_data`. To generate this data, we will reuse the functions `read_data()`, `create_dataset()`, and `resolve_missing()` from PA4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def numerify_instance(instance):\n",
    "    new = []\n",
    "    for attribute in instance:\n",
    "        try:\n",
    "            new.append(float(attribute))\n",
    "        except:\n",
    "            new.append(attribute)\n",
    "    return new\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset_r = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset_r.append(instance)\n",
    "    dataset = []\n",
    "    for instance in dataset_r:\n",
    "        newInstance = numerify_instance(instance)\n",
    "        dataset.append(newInstance)\n",
    "    return dataset\n",
    "\n",
    "def resolve_missing_values(data):\n",
    "    for i in range(10):\n",
    "        if i != 8:\n",
    "            sum_i = 0\n",
    "            count_i = 0\n",
    "            for instance in data:\n",
    "                if instance[i] != \"NA\":\n",
    "                    try:\n",
    "                        sum_i += instance[i]\n",
    "                        count_i += 1\n",
    "                    except:\n",
    "                        print(instance[i])\n",
    "            if count_i == 0:\n",
    "                continue\n",
    "            mean = sum_i / count_i\n",
    "            for instance in data:\n",
    "                if instance[i] == \"NA\":\n",
    "                    instance[i] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use these functions to populate `auto_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data_r = create_dataset(read_data(\"auto-data.txt\"))\n",
    "resolve_missing_values(auto_data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step only cares about the cyliders, weight, and model year attributes, as well as mpg as a classifier. So, to clean the dataset, we will first go through and restrict it to only these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_auto_data(data):\n",
    "    cleaned_auto_data = []\n",
    "    for instance in data:\n",
    "        cleaned_instance = [instance[1], instance[4], instance[6], instance[0]]\n",
    "        cleaned_auto_data.append(cleaned_instance)\n",
    "    return cleaned_auto_data\n",
    "\n",
    "auto_data_c = clean_auto_data(auto_data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will go through and discretize mpg based on the DOE classification ranking, as well as weight based on the NHTSA vehicle sizes classification. Both tables are given below for reference.\n",
    "\n",
    "| Rating | MPG   |\n",
    "|--------|-----  |\n",
    "|   10   | ≥ 45  |\n",
    "|   9    | 37-44 |\n",
    "|   8    | 31-36 |\n",
    "|   7    | 27-30 |\n",
    "|   6    | 24-26 |\n",
    "|   5    | 20-23 |\n",
    "|   4    | 17-19 |\n",
    "|   3    | 15-16 |\n",
    "|   2    |   14  |\n",
    "|   1    | ≤ 13  |\n",
    "\n",
    "| Ranking |  Weight   |\n",
    "|---------|-----------|\n",
    "|    5    | ≥ 3500    |\n",
    "|    4    | 3000-3499 |\n",
    "|    3    | 2500-2999 |\n",
    "|    2    | 2000-2499 |\n",
    "|    1    | ≤ 1999    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpg_to_DOE(mpg):\n",
    "    if mpg >= 45:\n",
    "        y = 10\n",
    "    elif mpg >= 37:\n",
    "        y = 9\n",
    "    elif mpg >= 31:\n",
    "        y = 8\n",
    "    elif mpg >= 27:\n",
    "        y = 7\n",
    "    elif mpg >= 24:\n",
    "        y = 6\n",
    "    elif mpg >= 20:\n",
    "        y = 5\n",
    "    elif mpg >= 17:\n",
    "        y = 4\n",
    "    elif mpg >= 15:\n",
    "        y = 3\n",
    "    elif mpg >= 14:\n",
    "        y = 2\n",
    "    else:\n",
    "        y = 1\n",
    "    return y\n",
    "\n",
    "def weight_to_NHTSA(weight):\n",
    "    if weight >= 3500:\n",
    "        return 5\n",
    "    elif weight >= 3000:\n",
    "        return 4\n",
    "    elif weight >= 2500:\n",
    "        return 3\n",
    "    elif weight >= 2000:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "def discretize_auto_data(data):\n",
    "    discrete_data = []\n",
    "    for instance in data:\n",
    "        discrete = [instance[0], weight_to_NHTSA(instance[1]), instance[2], mpg_to_DOE(instance[3])]\n",
    "        discrete_data.append(discrete)\n",
    "    return discrete_data\n",
    "\n",
    "auto_data = discretize_auto_data(auto_data_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test our classifier by repeating steps 2-5 from PA4.\n",
    "\n",
    "### Random Instances\n",
    "\n",
    "First, we will test our classifier on a subset of 5 random instances from the dataset. To do so, we must first generate 5 random instances to test. We will reuse `generate_random_instances()` from PA4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import copy\n",
    "\n",
    "def generate_random_instances(data, n):\n",
    "    data_c = copy.deepcopy(data)\n",
    "    test_instances = []\n",
    "    for i in range(n):\n",
    "        index = randint(0, len(data_c)-1)\n",
    "        instance = data_c.pop(index)\n",
    "        test_instances.append(instance)\n",
    "    return test_instances\n",
    "\n",
    "random_test = generate_random_instances(auto_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use our `naive_bayes_classify()` function to classify each test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(train, test):\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        predictions.append(naive_bayes_classify(train, 3, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], instance))\n",
    "    return predictions\n",
    "\n",
    "def print_output(test, predictions):\n",
    "    for x in range(len(test)):\n",
    "        print(\"instance: \", test[x][0], \", \", test[x][1], \", \", test[x][2], sep=\"\")\n",
    "        print(\"predicted: \", predictions[x], \", actual: \", test[x][3], sep=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can generate a list of predictions, we will run the classifier and output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: 4.0, 3, 72.0\n",
      "predicted: 5, actual: 4\n",
      "\n",
      "instance: 4.0, 2, 72.0\n",
      "predicted: 7, actual: 7\n",
      "\n",
      "instance: 4.0, 2, 79.0\n",
      "predicted: 7, actual: 8\n",
      "\n",
      "instance: 4.0, 1, 77.0\n",
      "predicted: 8, actual: 7\n",
      "\n",
      "instance: 8.0, 5, 76.0\n",
      "predicted: 1, actual: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = generate_predictions(auto_data, random_test)\n",
    "print_output(random_test, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test / Train sets\n",
    "\n",
    "For this test, we will use stratified cross validation to create a 2:1 train/test set. To generate the train and test sets, we will once again reuse functions from PA4: `create_random_subsample()`, `compute_accuracy()`, and `compute_error()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def create_random_subsample(data, size):\n",
    "    cutoff = int(len(data) * size)\n",
    "    data_c = copy.deepcopy(data)\n",
    "    shuffle(data_c)\n",
    "    train = data_c[:cutoff]\n",
    "    test = data_c[cutoff + 1:]\n",
    "    return test, train\n",
    "\n",
    "def compute_accuracy(predictions, actual):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == actual[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "def compute_error(predictions, actual):\n",
    "    incorrect = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != actual[i]:\n",
    "            incorrect += 1\n",
    "    error = incorrect / len(predictions)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use these functions to generate predictions and print the accuracy and the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.38461538461538464\n",
      "Error Rate:  0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "test, train = create_random_subsample(auto_data, 2/3)\n",
    "predictions = generate_predictions(train, test)\n",
    "accuracy = compute_accuracy(predictions, [x[3] for x in test])\n",
    "error = compute_error(predictions, [x[3] for x in test])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Error Rate: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Cross Validation\n",
    "\n",
    "Finally, we will use Stratified 10-Fold Cross Validation to generate random subsamples of test / train data to run our classifier on. \n",
    "\n",
    "We will borrow the function `create_cross_fold()` from PA4 for doing the subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_fold(data, n):\n",
    "    data_r = copy.deepcopy(data)\n",
    "    shuffle(data_r)\n",
    "    size = int(len(data) * 1/n) \n",
    "    start = 0\n",
    "    end = size\n",
    "    folds = []\n",
    "    for i in range(n-1):\n",
    "        folds.append(data[start:end])\n",
    "        start = end + 1\n",
    "        end += size + 1\n",
    "    folds.append(data[start:])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to run the classifier 10 times, using each subsequent fold as test data and the other 9 folds as training data. For each fold, we will compute an accuracy and error rate, and then display the average at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3471923536439666\n",
      "Error Rate: 0.6528076463560335\n"
     ]
    }
   ],
   "source": [
    "def print_cross_fold_output(data, n):\n",
    "    sum_accuracy = 0\n",
    "    sum_error = 0\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = generate_predictions(train, test)\n",
    "        sum_accuracy += compute_accuracy(predictions, [x[3] for x in test])\n",
    "        sum_error += compute_error(predictions, [x[3] for x in test])\n",
    "    accuracy = sum_accuracy / n\n",
    "    error = sum_error / n\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Error Rate:\", error)\n",
    "    \n",
    "print_cross_fold_output(auto_data, 10)"
   ]
  },
  {
   "attachments": {
    "math.svg": {
     "image/svg+xml": [
      "<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="137.112255pt" height="33.986782pt" viewBox="0 0 137.112255 33.986782" version="1.1">
<defs>
<g>
<symbol overflow="visible" id="glyph0-0">
<path style="stroke:none;" d="M 5.65625 -4.390625 L 5.65625 -4.859375 L 4.9375 -4.859375 C 4.890625 -4.859375 4.84375 -4.890625 4.734375 -4.953125 C 4.421875 -5.1875 4.015625 -5.296875 3.5625 -5.296875 C 2.359375 -5.296875 1.15625 -4.46875 1.15625 -3.234375 C 1.15625 -2.609375 1.421875 -2.1875 2.09375 -1.9375 C 1.453125 -1.640625 1.171875 -1.28125 1.171875 -0.9375 C 1.171875 -0.78125 1.28125 -0.59375 1.5 -0.46875 C 0.328125 0.234375 0.09375 0.5625 0.09375 1.171875 C 0.09375 2.046875 0.875 2.46875 2.09375 2.46875 C 3.703125 2.46875 4.625 1.703125 4.625 0.65625 C 4.625 -0.0625 4.0625 -0.59375 2.921875 -0.859375 C 2.390625 -1 2.0625 -1.15625 2.0625 -1.375 C 2.0625 -1.625 2.296875 -1.859375 2.5 -1.859375 C 2.515625 -1.859375 2.546875 -1.859375 2.578125 -1.84375 C 2.6875 -1.828125 2.8125 -1.8125 2.953125 -1.8125 C 4.0625 -1.8125 5.171875 -2.765625 5.171875 -3.78125 C 5.171875 -3.984375 5.171875 -4.171875 5.0625 -4.390625 Z M 4.21875 -4.171875 C 4.21875 -3.671875 4.0625 -3.1875 3.828125 -2.765625 C 3.546875 -2.28125 3.21875 -2.0625 2.8125 -2.0625 C 2.359375 -2.0625 2.09375 -2.40625 2.09375 -2.96875 C 2.09375 -4.15625 2.8125 -5.03125 3.546875 -5.03125 C 4 -5.03125 4.21875 -4.8125 4.21875 -4.171875 Z M 3.90625 1.078125 C 3.90625 1.6875 3.25 2.203125 2.28125 2.203125 C 1.3125 2.203125 0.75 1.828125 0.75 0.984375 C 0.75 0.703125 0.8125 0.515625 1 0.28125 C 1.15625 0.078125 1.671875 -0.34375 1.78125 -0.34375 C 1.90625 -0.28125 2.03125 -0.25 2.1875 -0.203125 C 3.46875 0.1875 3.90625 0.53125 3.90625 1.078125 Z M 3.90625 1.078125 "/>
</symbol>
<symbol overflow="visible" id="glyph0-1">
<path style="stroke:none;" d="M 2.921875 -4.265625 C 2.75 -5.078125 2.609375 -5.296875 2.296875 -5.296875 C 2.03125 -5.296875 1.625 -5.203125 0.90625 -4.9375 L 0.765625 -4.890625 L 0.796875 -4.703125 L 1.015625 -4.765625 C 1.25 -4.828125 1.390625 -4.84375 1.484375 -4.84375 C 1.78125 -4.84375 1.875 -4.75 2.046875 -4.03125 L 2.375 -2.546875 L 1.390625 -1.15625 C 1.140625 -0.796875 0.90625 -0.5625 0.78125 -0.5625 C 0.703125 -0.5625 0.59375 -0.59375 0.46875 -0.671875 C 0.3125 -0.75 0.1875 -0.796875 0.078125 -0.796875 C -0.15625 -0.796875 -0.328125 -0.609375 -0.328125 -0.375 C -0.328125 -0.0625 -0.09375 0.125 0.28125 0.125 C 0.640625 0.125 0.890625 0.03125 1.421875 -0.6875 L 2.46875 -2.109375 L 2.8125 -0.6875 C 2.96875 -0.078125 3.140625 0.125 3.53125 0.125 C 3.984375 0.125 4.296875 -0.15625 4.984375 -1.234375 L 4.8125 -1.34375 C 4.71875 -1.21875 4.671875 -1.15625 4.5625 -1.015625 C 4.28125 -0.640625 4.15625 -0.53125 4 -0.53125 C 3.828125 -0.53125 3.71875 -0.6875 3.640625 -1.015625 L 3.25 -2.625 C 3.1875 -2.921875 3.15625 -3.078125 3.15625 -3.171875 C 3.6875 -4.09375 4.109375 -4.625 4.328125 -4.625 C 4.625 -4.625 4.734375 -4.421875 4.953125 -4.421875 C 5.203125 -4.421875 5.359375 -4.59375 5.359375 -4.84375 C 5.359375 -5.109375 5.15625 -5.296875 4.875 -5.296875 C 4.34375 -5.296875 3.90625 -4.859375 3.0625 -3.578125 Z M 2.921875 -4.265625 "/>
</symbol>
<symbol overflow="visible" id="glyph0-2">
<path style="stroke:none;" d="M 5.625 -1.40625 C 5 -0.59375 4.828125 -0.4375 4.625 -0.4375 C 4.546875 -0.4375 4.5 -0.515625 4.5 -0.609375 C 4.5 -0.734375 4.6875 -1.34375 4.734375 -1.53125 L 5.671875 -5.140625 L 4.75 -5.140625 L 4.359375 -3.671875 C 4.15625 -2.90625 2.84375 -0.65625 2.140625 -0.65625 C 1.796875 -0.65625 1.65625 -0.90625 1.65625 -1.234375 C 1.65625 -1.34375 1.6875 -1.453125 1.703125 -1.5625 L 2.609375 -5.140625 L 1.703125 -5.140625 L 0.125 1.109375 C -0.0625 1.875 -0.234375 2.21875 -0.390625 2.453125 L 0.453125 2.453125 C 0.71875 2.140625 0.796875 1.578125 1.265625 -0.234375 C 1.3125 -0.015625 1.46875 0.109375 1.6875 0.109375 C 2.40625 0.109375 3.078125 -0.703125 4.125 -2.5625 L 4.140625 -2.5625 L 3.75 -1.3125 C 3.640625 -1.03125 3.5625 -0.609375 3.5625 -0.40625 C 3.5625 -0.078125 3.8125 0.125 4.15625 0.125 C 4.6875 0.125 5.046875 -0.140625 5.796875 -1.296875 Z M 5.625 -1.40625 "/>
</symbol>
<symbol overflow="visible" id="glyph0-3">
<path style="stroke:none;" d="M 6.375 -5.140625 L 4.046875 -5.140625 C 1.78125 -5.140625 0.328125 -3.1875 0.328125 -1.5625 C 0.328125 -0.515625 0.953125 0.125 2.078125 0.125 C 3.75 0.125 5.265625 -1.28125 5.265625 -2.609375 C 5.265625 -3.5 4.59375 -3.484375 4.109375 -4.25 L 6.15625 -4.25 Z M 4.265625 -2.65625 C 4.265625 -1.6875 3.421875 -0.125 2.265625 -0.125 C 1.703125 -0.125 1.328125 -0.484375 1.328125 -1.203125 C 1.328125 -2.890625 2.734375 -4.25 3.5625 -4.25 C 3.953125 -4.015625 4.265625 -3.421875 4.265625 -2.65625 Z M 4.265625 -2.65625 "/>
</symbol>
<symbol overflow="visible" id="glyph0-4">
<path style="stroke:none;" d="M 4.296875 -1.3125 C 3.4375 -0.59375 3.0625 -0.40625 2.53125 -0.40625 C 1.859375 -0.40625 1.421875 -0.84375 1.421875 -1.5 C 1.421875 -1.6875 1.421875 -1.984375 1.53125 -2.234375 C 3.6875 -2.515625 4.9375 -3.4375 4.9375 -4.46875 C 4.9375 -4.984375 4.59375 -5.296875 3.953125 -5.296875 C 2.15625 -5.296875 0.375 -3.375 0.375 -1.515625 C 0.375 -0.515625 1.015625 0.125 1.984375 0.125 C 2.984375 0.125 3.75 -0.328125 4.4375 -1.15625 Z M 1.828125 -3.03125 C 2.234375 -4.09375 3.046875 -5.015625 3.734375 -5.015625 C 4.0625 -5.015625 4.15625 -4.84375 4.15625 -4.546875 C 4.15625 -4.171875 3.921875 -3.75 3.546875 -3.390625 C 3.109375 -2.96875 2.65625 -2.75 1.625 -2.5 Z M 1.828125 -3.03125 "/>
</symbol>
<symbol overflow="visible" id="glyph1-0">
<path style="stroke:none;" d="M 4.15625 11.578125 C 3.28125 10.878906 2.632812 10.003906 2.21875 8.953125 C 1.8125 7.910156 1.609375 6.789062 1.609375 5.59375 C 1.609375 4.394531 1.8125 3.269531 2.21875 2.21875 C 2.632812 1.164062 3.28125 0.300781 4.15625 -0.375 C 4.15625 -0.394531 4.175781 -0.40625 4.21875 -0.40625 L 4.34375 -0.40625 C 4.363281 -0.40625 4.378906 -0.394531 4.390625 -0.375 C 4.410156 -0.351562 4.421875 -0.332031 4.421875 -0.3125 C 4.421875 -0.28125 4.414062 -0.257812 4.40625 -0.25 C 4.019531 0.125 3.695312 0.539062 3.4375 1 C 3.175781 1.457031 2.96875 1.929688 2.8125 2.421875 C 2.664062 2.921875 2.554688 3.4375 2.484375 3.96875 C 2.421875 4.507812 2.390625 5.054688 2.390625 5.609375 C 2.390625 8.191406 3.0625 10.132812 4.40625 11.4375 C 4.414062 11.445312 4.421875 11.46875 4.421875 11.5 C 4.421875 11.519531 4.410156 11.539062 4.390625 11.5625 C 4.378906 11.582031 4.363281 11.59375 4.34375 11.59375 L 4.21875 11.59375 C 4.175781 11.59375 4.15625 11.585938 4.15625 11.578125 Z M 4.15625 11.578125 "/>
</symbol>
<symbol overflow="visible" id="glyph2-0">
<path style="stroke:none;" d="M 1 1.6875 C 1.828125 1.296875 2.34375 0.546875 2.34375 -0.1875 C 2.34375 -0.796875 1.921875 -1.21875 1.375 -1.21875 C 0.953125 -1.21875 0.65625 -0.953125 0.65625 -0.546875 C 0.65625 -0.125 0.90625 0.078125 1.359375 0.078125 C 1.484375 0.078125 1.609375 0.03125 1.703125 0.03125 C 1.796875 0.03125 1.875 0.09375 1.875 0.1875 C 1.875 0.578125 1.53125 1.015625 0.875 1.46875 Z M 1 1.6875 "/>
</symbol>
<symbol overflow="visible" id="glyph2-1">
<path style="stroke:none;" d="M 7.640625 -3.84375 L 7.640625 -4.625 L 0.578125 -4.625 L 0.578125 -3.84375 Z M 7.640625 -1.4375 L 7.640625 -2.234375 L 0.578125 -2.234375 L 0.578125 -1.4375 Z M 7.640625 -1.4375 "/>
</symbol>
<symbol overflow="visible" id="glyph3-0">
<path style="stroke:none;" d="M 0.53125 11.59375 C 0.46875 11.59375 0.4375 11.5625 0.4375 11.5 C 0.4375 11.46875 0.445312 11.445312 0.46875 11.4375 C 0.976562 10.9375 1.378906 10.367188 1.671875 9.734375 C 1.972656 9.109375 2.1875 8.445312 2.3125 7.75 C 2.4375 7.050781 2.5 6.332031 2.5 5.59375 C 2.5 3 1.820312 1.050781 0.46875 -0.25 C 0.445312 -0.257812 0.4375 -0.28125 0.4375 -0.3125 C 0.4375 -0.375 0.46875 -0.40625 0.53125 -0.40625 L 0.671875 -0.40625 C 0.691406 -0.40625 0.707031 -0.394531 0.71875 -0.375 C 1.601562 0.300781 2.25 1.164062 2.65625 2.21875 C 3.070312 3.269531 3.28125 4.394531 3.28125 5.59375 C 3.28125 6.789062 3.070312 7.910156 2.65625 8.953125 C 2.25 10.003906 1.601562 10.878906 0.71875 11.578125 C 0.707031 11.585938 0.691406 11.59375 0.671875 11.59375 Z M 0.53125 11.59375 "/>
</symbol>
<symbol overflow="visible" id="glyph4-0">
<path style="stroke:none;" d="M 3.359375 0 L 3.359375 -0.125 C 2.71875 -0.125 2.546875 -0.28125 2.546875 -0.640625 L 2.546875 -5.734375 L 2.46875 -5.75 L 0.9375 -4.984375 L 0.9375 -4.859375 L 1.171875 -4.953125 C 1.328125 -5 1.46875 -5.046875 1.5625 -5.046875 C 1.734375 -5.046875 1.8125 -4.921875 1.8125 -4.625 L 1.8125 -0.8125 C 1.8125 -0.34375 1.640625 -0.15625 1 -0.125 L 1 0 Z M 3.359375 0 "/>
</symbol>
<symbol overflow="visible" id="glyph4-1">
<path style="stroke:none;" d="M 4.03125 -1.171875 L 3.921875 -1.21875 C 3.640625 -0.734375 3.453125 -0.640625 3.09375 -0.640625 L 1.109375 -0.640625 L 2.515625 -2.140625 C 3.265625 -2.953125 3.609375 -3.578125 3.609375 -4.25 C 3.609375 -5.09375 2.984375 -5.75 2.03125 -5.75 C 0.984375 -5.75 0.4375 -5.0625 0.25 -4.0625 L 0.4375 -4.015625 C 0.78125 -4.859375 1.078125 -5.125 1.6875 -5.125 C 2.40625 -5.125 2.875 -4.703125 2.875 -3.921875 C 2.875 -3.203125 2.5625 -2.546875 1.765625 -1.71875 L 0.25 -0.109375 L 0.25 0 L 3.578125 0 Z M 4.03125 -1.171875 "/>
</symbol>
<symbol overflow="visible" id="glyph4-2">
<path style="stroke:none;" d="M 5.28125 -1.875 L 5.28125 -2.4375 L 0.546875 -2.4375 L 0.546875 -1.875 Z M 5.28125 -1.875 "/>
</symbol>
<symbol overflow="visible" id="glyph5-0">
<path style="stroke:none;" d="M 3.609375 10.828125 L 1.671875 5.953125 L 1.09375 6.4375 L 0.9375 6.328125 L 2.15625 5.296875 L 3.96875 9.875 L 8.359375 -0.171875 C 8.378906 -0.222656 8.429688 -0.25 8.515625 -0.25 C 8.566406 -0.25 8.609375 -0.234375 8.640625 -0.203125 C 8.671875 -0.179688 8.6875 -0.15625 8.6875 -0.125 L 8.6875 -0.09375 L 3.9375 10.765625 C 3.90625 10.804688 3.859375 10.828125 3.796875 10.828125 Z M 3.609375 10.828125 "/>
</symbol>
<symbol overflow="visible" id="glyph6-0">
<path style="stroke:none;" d="M 4.5625 -3.640625 L 2.25 -3.640625 C 1.390625 -3.640625 0.953125 -3.328125 0.5 -2.546875 L 0.625 -2.546875 C 1 -3 1.234375 -3.015625 1.671875 -3.015625 L 1.734375 -3.015625 L 1.03125 -1.234375 C 0.9375 -1.015625 0.78125 -0.734375 0.421875 -0.640625 C 0.203125 -0.578125 0.15625 -0.34375 0.15625 -0.21875 C 0.15625 -0.015625 0.359375 0.15625 0.59375 0.15625 C 0.6875 0.15625 0.796875 0.125 0.890625 0.046875 C 1.578125 -0.5 1.8125 -1.96875 2.1875 -3.015625 L 3.078125 -3.015625 L 2.421875 -1.21875 C 2.375 -1.0625 2.28125 -0.734375 2.28125 -0.546875 C 2.28125 -0.21875 2.421875 0.09375 2.78125 0.09375 C 3.421875 0.09375 3.828125 -0.59375 3.953125 -1.109375 L 3.8125 -1.109375 C 3.609375 -0.65625 3.375 -0.59375 3.203125 -0.59375 C 3.03125 -0.59375 2.953125 -0.765625 2.953125 -0.953125 C 2.953125 -1.125 3.390625 -2.59375 3.515625 -3.015625 L 4.390625 -3.015625 Z M 4.5625 -3.640625 "/>
</symbol>
<symbol overflow="visible" id="glyph6-1">
<path style="stroke:none;" d="M 4.515625 -3.640625 L 2.875 -3.640625 C 1.265625 -3.640625 0.234375 -2.25 0.234375 -1.109375 C 0.234375 -0.359375 0.671875 0.09375 1.46875 0.09375 C 2.671875 0.09375 3.734375 -0.90625 3.734375 -1.84375 C 3.734375 -2.484375 3.265625 -2.484375 2.921875 -3.015625 L 4.375 -3.015625 Z M 3.015625 -1.875 C 3.015625 -1.203125 2.421875 -0.078125 1.609375 -0.078125 C 1.203125 -0.078125 0.9375 -0.34375 0.9375 -0.84375 C 0.9375 -2.046875 1.9375 -3.015625 2.53125 -3.015625 C 2.796875 -2.859375 3.015625 -2.421875 3.015625 -1.875 Z M 3.015625 -1.875 "/>
</symbol>
<symbol overflow="visible" id="glyph7-0">
<path style="stroke:none;" d="M 2.78125 7.703125 C 2.195312 7.242188 1.765625 6.664062 1.484375 5.96875 C 1.210938 5.269531 1.078125 4.523438 1.078125 3.734375 C 1.078125 2.929688 1.210938 2.175781 1.484375 1.46875 C 1.765625 0.769531 2.195312 0.195312 2.78125 -0.25 C 2.78125 -0.257812 2.789062 -0.265625 2.8125 -0.265625 L 2.90625 -0.265625 C 2.914062 -0.265625 2.925781 -0.253906 2.9375 -0.234375 C 2.945312 -0.222656 2.953125 -0.210938 2.953125 -0.203125 C 2.953125 -0.179688 2.945312 -0.171875 2.9375 -0.171875 C 2.6875 0.078125 2.472656 0.351562 2.296875 0.65625 C 2.117188 0.96875 1.976562 1.285156 1.875 1.609375 C 1.78125 1.941406 1.707031 2.285156 1.65625 2.640625 C 1.613281 3.003906 1.59375 3.367188 1.59375 3.734375 C 1.59375 5.453125 2.039062 6.742188 2.9375 7.609375 C 2.945312 7.617188 2.953125 7.632812 2.953125 7.65625 C 2.953125 7.664062 2.945312 7.675781 2.9375 7.6875 C 2.925781 7.707031 2.914062 7.71875 2.90625 7.71875 L 2.8125 7.71875 C 2.789062 7.71875 2.78125 7.710938 2.78125 7.703125 Z M 2.78125 7.703125 "/>
</symbol>
<symbol overflow="visible" id="glyph7-1">
<path style="stroke:none;" d="M 0.359375 7.71875 C 0.316406 7.71875 0.296875 7.695312 0.296875 7.65625 C 0.296875 7.632812 0.300781 7.617188 0.3125 7.609375 C 0.644531 7.285156 0.910156 6.910156 1.109375 6.484375 C 1.304688 6.066406 1.445312 5.625 1.53125 5.15625 C 1.613281 4.695312 1.65625 4.21875 1.65625 3.71875 C 1.65625 2 1.207031 0.703125 0.3125 -0.171875 C 0.300781 -0.171875 0.296875 -0.179688 0.296875 -0.203125 C 0.296875 -0.242188 0.316406 -0.265625 0.359375 -0.265625 L 0.4375 -0.265625 C 0.457031 -0.265625 0.46875 -0.257812 0.46875 -0.25 C 1.050781 0.195312 1.476562 0.769531 1.75 1.46875 C 2.03125 2.175781 2.171875 2.929688 2.171875 3.734375 C 2.171875 4.523438 2.035156 5.269531 1.765625 5.96875 C 1.492188 6.664062 1.0625 7.242188 0.46875 7.703125 C 0.46875 7.710938 0.457031 7.71875 0.4375 7.71875 Z M 0.359375 7.71875 "/>
</symbol>
<symbol overflow="visible" id="glyph8-0">
<path style="stroke:none;" d="M 1.9375 -2.84375 C 1.828125 -3.390625 1.734375 -3.53125 1.53125 -3.53125 C 1.359375 -3.53125 1.09375 -3.46875 0.59375 -3.296875 L 0.515625 -3.265625 L 0.53125 -3.140625 L 0.6875 -3.171875 C 0.828125 -3.21875 0.921875 -3.234375 0.984375 -3.234375 C 1.1875 -3.234375 1.25 -3.171875 1.359375 -2.6875 L 1.578125 -1.703125 L 0.921875 -0.765625 C 0.765625 -0.53125 0.609375 -0.375 0.515625 -0.375 C 0.46875 -0.375 0.390625 -0.40625 0.3125 -0.453125 C 0.203125 -0.5 0.125 -0.53125 0.0625 -0.53125 C -0.109375 -0.53125 -0.21875 -0.40625 -0.21875 -0.25 C -0.21875 -0.046875 -0.0625 0.09375 0.1875 0.09375 C 0.4375 0.09375 0.59375 0.015625 0.9375 -0.453125 L 1.640625 -1.40625 L 1.875 -0.453125 C 1.96875 -0.0625 2.09375 0.09375 2.359375 0.09375 C 2.65625 0.09375 2.859375 -0.109375 3.328125 -0.828125 L 3.203125 -0.890625 C 3.140625 -0.8125 3.109375 -0.765625 3.046875 -0.671875 C 2.859375 -0.4375 2.765625 -0.359375 2.65625 -0.359375 C 2.546875 -0.359375 2.484375 -0.453125 2.421875 -0.6875 L 2.171875 -1.75 C 2.125 -1.9375 2.109375 -2.0625 2.109375 -2.109375 C 2.453125 -2.734375 2.75 -3.078125 2.890625 -3.078125 C 3.078125 -3.078125 3.15625 -2.9375 3.296875 -2.9375 C 3.46875 -2.9375 3.578125 -3.0625 3.578125 -3.234375 C 3.578125 -3.40625 3.4375 -3.53125 3.25 -3.53125 C 2.890625 -3.53125 2.59375 -3.234375 2.046875 -2.390625 Z M 1.9375 -2.84375 "/>
</symbol>
<symbol overflow="visible" id="glyph8-1">
<path style="stroke:none;" d="M 3.75 -0.9375 C 3.34375 -0.390625 3.21875 -0.28125 3.09375 -0.28125 C 3.03125 -0.28125 3 -0.34375 3 -0.40625 C 3 -0.484375 3.125 -0.890625 3.15625 -1.03125 L 3.78125 -3.421875 L 3.171875 -3.421875 L 2.90625 -2.453125 C 2.765625 -1.9375 1.890625 -0.4375 1.421875 -0.4375 C 1.203125 -0.4375 1.109375 -0.609375 1.109375 -0.828125 C 1.109375 -0.890625 1.125 -0.96875 1.140625 -1.046875 L 1.734375 -3.421875 L 1.140625 -3.421875 L 0.09375 0.75 C -0.046875 1.25 -0.15625 1.484375 -0.265625 1.640625 L 0.296875 1.640625 C 0.484375 1.4375 0.53125 1.046875 0.84375 -0.15625 C 0.875 -0.015625 0.96875 0.078125 1.125 0.078125 C 1.609375 0.078125 2.046875 -0.46875 2.75 -1.71875 L 2.765625 -1.71875 L 2.5 -0.875 C 2.4375 -0.6875 2.375 -0.40625 2.375 -0.265625 C 2.375 -0.046875 2.546875 0.09375 2.765625 0.09375 C 3.125 0.09375 3.375 -0.09375 3.859375 -0.859375 Z M 3.75 -0.9375 "/>
</symbol>
<symbol overflow="visible" id="glyph8-2">
<path style="stroke:none;" d="M 4.25 -3.421875 L 2.703125 -3.421875 C 1.1875 -3.421875 0.21875 -2.125 0.21875 -1.046875 C 0.21875 -0.34375 0.625 0.09375 1.390625 0.09375 C 2.5 0.09375 3.515625 -0.859375 3.515625 -1.734375 C 3.515625 -2.34375 3.0625 -2.328125 2.75 -2.828125 L 4.109375 -2.828125 Z M 2.84375 -1.765625 C 2.84375 -1.125 2.28125 -0.078125 1.515625 -0.078125 C 1.140625 -0.078125 0.890625 -0.3125 0.890625 -0.796875 C 0.890625 -1.921875 1.828125 -2.828125 2.375 -2.828125 C 2.625 -2.6875 2.84375 -2.28125 2.84375 -1.765625 Z M 2.84375 -1.765625 "/>
</symbol>
<symbol overflow="visible" id="glyph9-0">
<path style="stroke:none;" d="M 4.96875 -1.765625 L 4.96875 -2.28125 L 0.515625 -2.28125 L 0.515625 -1.765625 Z M 4.96875 -1.765625 "/>
</symbol>
<symbol overflow="visible" id="glyph9-1">
<path style="stroke:none;" d="M 3.796875 -1.09375 L 3.6875 -1.140625 C 3.421875 -0.703125 3.25 -0.609375 2.90625 -0.609375 L 1.046875 -0.609375 L 2.359375 -2.015625 C 3.078125 -2.765625 3.390625 -3.375 3.390625 -4 C 3.390625 -4.796875 2.8125 -5.40625 1.90625 -5.40625 C 0.921875 -5.40625 0.40625 -4.75 0.234375 -3.8125 L 0.40625 -3.78125 C 0.734375 -4.5625 1.015625 -4.8125 1.578125 -4.8125 C 2.265625 -4.8125 2.703125 -4.421875 2.703125 -3.6875 C 2.703125 -3.015625 2.40625 -2.40625 1.65625 -1.609375 L 0.234375 -0.09375 L 0.234375 0 L 3.359375 0 Z M 3.796875 -1.09375 "/>
</symbol>
</g>
</defs>
<g id="surface1">
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph0-0" x="0.574219" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph1-0" x="5.941406" y="15.392724"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph0-1" x="12.035156" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph2-0" x="18.074219" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph0-2" x="26.140625" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph2-0" x="32.613281" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph0-3" x="39.964844" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph3-0" x="47.222656" y="15.392724"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph2-1" x="54.589844" y="24.027344"/>
</g>
<path style="fill:none;stroke-width:1.2;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;" d="M 67.5625 -3.038644 L 90.621094 -3.038644 " transform="matrix(1,0,0,1,0,24.026925)"/>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph4-0" x="76.945312" y="18.125"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph5-0" x="66.625" y="23.17001"/>
</g>
<path style="fill:none;stroke-width:0.426;stroke-linecap:round;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;" d="M 75.097656 -0.894113 L 85.179688 -0.894113 " transform="matrix(1,0,0,1,0,24.026925)"/>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph4-1" x="75.535156" y="31.464844"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph6-0" x="80.359375" y="31.464844"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph6-1" x="85.632812" y="31.464844"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph0-4" x="92.246094" y="24.027344"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph4-2" x="99.109375" y="14.074219"/>
</g>
<path style="fill:none;stroke-width:1.2;stroke-linecap:butt;stroke-linejoin:miter;stroke:rgb(0%,0%,0%);stroke-opacity:1;stroke-miterlimit:10;" d="M 105.8125 -12.108956 L 136.164062 -12.108956 " transform="matrix(1,0,0,1,0,24.026925)"/>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph7-0" x="105.175781" y="2.64537"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph8-0" x="109.238281" y="8.393555"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph9-0" x="115.855469" y="8.393555"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph8-1" x="124.648438" y="8.393555"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph7-1" x="129.101562" y="2.64537"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph9-1" x="131.925781" y="5.405273"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph9-1" x="114.289062" y="20.893555"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph8-2" x="118.753906" y="20.893555"/>
</g>
<g style="fill:rgb(0%,0%,0%);fill-opacity:1;">
  <use xlink:href="#glyph9-1" x="123.660156" y="18.870117"/>
</g>
</g>
</svg>
"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Gaussian Distribution\n",
    "\n",
    "For this step, we will repeat step 2 treating weight as a continuous attribute. To do this, we will utilize the Gaussian distribution function to calculate its conditional probability.\n",
    "\n",
    "First, we will implement the `gaussian()` functions, referenced from [U4 - Supervised Learning](https://github.com/GonzagaCPSC310/U4-Supervised-Learning/blob/master/D%20Naive%20Bayes.ipynb), defined by:\n",
    "\n",
    "![math.svg](attachment:math.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def gaussian(x, mean, sdev):\n",
    "  first, second = 0, 0\n",
    "  if sdev > 0:\n",
    "      first = 1 / (math.sqrt(2 * math.pi) * sdev)\n",
    "      second = math.e ** (-((x - mean) ** 2) / (2 * (sdev ** 2)))\n",
    "  return first * second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now redefine our classifier, using the Gaussian function for weight. \n",
    "\n",
    "We will also need helper functions in order to calculate the mean and standard deviation of a given attribute for a given class label. \n",
    "\n",
    "\n",
    "* `class_std_dev_attribute()`\n",
    "    * **Params**:\n",
    "        * `data` - The dataset to calculate standard deviation over\n",
    "        * `index` - The index of the attribute to calculate standard deviation for\n",
    "        * `classLabel` - The class label to calculate the std dev for\n",
    "        * `classIndex` - the index of the classifier attribute in the data\n",
    "    * **Returns**:\n",
    "        * The standard deviation of the given attribute in the dataset over all instances where the class label is `classLabel`\n",
    "* `class_mean_attribute()`\n",
    "    * **Params**:\n",
    "        * `data` - The dataset to calculate mean over\n",
    "        * `index` - The index of the attribute to calculate mean for\n",
    "        * `classLabel` - The class label to calculate the mean for\n",
    "        * `classIndex` - the index of the classifier attribute in the data\n",
    "    * **Returns**:\n",
    "        * The mean of the given attriute over all instances where the class label is `classLabel`\n",
    "* `naive_bayes_classify_gaussian()`\n",
    "    * **Params**:\n",
    "        * `train` - The dataset to use for the training set\n",
    "        * `classIndex` - The index of the classifier attribute within the data\n",
    "        * `classLabels` - A list of all possible class labels\n",
    "        * `gaussianIndices` - A list of indices to treat as continuous variables and use gaussian distributions on\n",
    "        * `test` - The unseen test data to predict a class for\n",
    "    * **Returns**:\n",
    "        * A predicted classification using Naive Bayes, calculated using posterior probabilities for categorical variables and gaussian distributions for continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_mean_attribute(data, index, classLabel, classIndex):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for instance in data:\n",
    "        if instance[classIndex] == classLabel:\n",
    "            sum += instance[classIndex]\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return sum / count\n",
    "\n",
    "def class_std_dev_attribute(data, index, classLabel, classIndex):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    mean = class_mean_attribute(data, index, classLabel, classIndex)\n",
    "    for instance in data:\n",
    "        if instance[classIndex] == classLabel:\n",
    "            sum += (instance[index] - mean)**2\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return ((sum / count)**(1/2))\n",
    "\n",
    "\n",
    "def naive_bayes_classify_gaussian(train, classIndex, classLabels, gaussianIndices, test):\n",
    "    classProbabilities = []\n",
    "    for val in classLabels:\n",
    "        classProbabilities.append(0)\n",
    "    priors = calculate_priors(train, classIndex, classLabels)\n",
    "    for i in range(len(classProbabilities)):\n",
    "        classProbabilities[i] += priors[i]\n",
    "    for i in range(len(test)):\n",
    "        if i == classIndex:\n",
    "            continue\n",
    "        elif i in gaussianIndices:\n",
    "            gaussians = []\n",
    "            for label in classLabels:\n",
    "                mean = class_mean_attribute(train, i, label, classIndex)\n",
    "                std_dev = class_std_dev_attribute(train, i, label, classIndex)\n",
    "                gaussians.append(gaussian(test[i], mean, std_dev))\n",
    "            for i in range(len(gaussians)):\n",
    "                classProbabilities[i] *= gaussians[i]\n",
    "        else:\n",
    "            attribute = test[i]\n",
    "            posteriors = calculate_posteriors(train, i, attribute, classIndex, classLabels)\n",
    "            for i in range(len(posteriors)):\n",
    "                classProbabilities[i] *= posteriors[i]\n",
    "    maxP = 0\n",
    "    index = 0\n",
    "    for i in range(len(classProbabilities)):\n",
    "        if classProbabilities[i] > maxP:\n",
    "            maxP = classProbabilities[i]\n",
    "            index = i\n",
    "    return classLabels[index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to recreate the dataset, since weight is no longer a categorical variable. We will copy the previous `discretize_auto_data()` function, leaving weight as is instead of converting to NHTSA rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_auto_data_gaussian(data):\n",
    "    discrete_data = []\n",
    "    for instance in data:\n",
    "        discrete = [instance[0], instance[1], instance[2], mpg_to_DOE(instance[3])]\n",
    "        discrete_data.append(discrete)\n",
    "    return discrete_data\n",
    "\n",
    "auto_data_g = discretize_auto_data_gaussian(auto_data_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now recreate the phases of Step 2, using this new gaussian classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Instances\n",
    "\n",
    "First, we will test the Gaussian classifier against 5 random instances. We must modify our `generate_predictions()` function in order to use the gaussian classifier, which we will rename `generate_gaussian_predictions()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian_predictions(train, test):\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        predictions.append(naive_bayes_classify_gaussian(train, 3, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [1], instance))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the same random generation function and prediction output function as before to test our gaussian prediction on a random subsample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance: 4.0, 2694.0, 75.0\n",
      "predicted: 6, actual: 5\n",
      "\n",
      "instance: 4.0, 2155.0, 76.0\n",
      "predicted: 7, actual: 7\n",
      "\n",
      "instance: 6.0, 2904.0, 73.0\n",
      "predicted: 4, actual: 5\n",
      "\n",
      "instance: 4.0, 2130.0, 70.0\n",
      "predicted: 6, actual: 7\n",
      "\n",
      "instance: 8.0, 4440.0, 75.0\n",
      "predicted: 3, actual: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_test_g = generate_random_instances(auto_data_g, 5)\n",
    "predictions_g = generate_gaussian_predictions(auto_data_g, random_test_g)\n",
    "print_output(random_test_g, predictions_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Sets: Random Sub-Sampling\n",
    "\n",
    "Second, we will once again generate a random 2:1 split of train / test data and perform our prediction using the gaussian classifier. This will follow the exact same format as the original classifier, using the new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3076923076923077\n",
      "Error Rate:  0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "test, train = create_random_subsample(auto_data_g, 2/3)\n",
    "predictions = generate_gaussian_predictions(train, test)\n",
    "accuracy = compute_accuracy(predictions, [x[3] for x in test])\n",
    "error = compute_error(predictions, [x[3] for x in test])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Error Rate: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Sets: Stratified Cross Validation\n",
    "\n",
    "Finally, we will recreate our 10-fold cross validation testing using the gaussian classifier. We will reuse the `print_cross_fold_output()` function, rewritten to use our gaussian classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3097968936678614\n",
      "Error Rate: 0.6902031063321387\n"
     ]
    }
   ],
   "source": [
    "def print_cross_fold_output_gaussian(data, n):\n",
    "    sum_accuracy = 0\n",
    "    sum_error = 0\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = generate_gaussian_predictions(train, test)\n",
    "        sum_accuracy += compute_accuracy(predictions, [x[3] for x in test])\n",
    "        sum_error += compute_error(predictions, [x[3] for x in test])\n",
    "    accuracy = sum_accuracy / n\n",
    "    error = sum_error / n\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Error Rate:\", error)\n",
    "\n",
    "\n",
    "print_cross_fold_output_gaussian(auto_data_g, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Titanic Dataset\n",
    "\n",
    "For this step, we will use both Naive Bayes and k-NN to predict the survival attribute from the titanic dataset, which we will now load. Note that this file has been cleaned from its origial form to only include the relevant data, without any headers or identifying information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = create_dataset(read_data(\"titanic_data.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define 2 functions similar to functions used previously: `generate_titanic_predictions()` and `print_titanic_cross_fold_output()`. These will allow us to use the Naive Bayes classifier on the titanic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_titanic_predictions(train, test):\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        predictions.append(naive_bayes_classify(train, 3, [\"yes\", \"no\"], instance))\n",
    "    return predictions\n",
    "\n",
    "def print_titanic_cross_fold_output(data, n):\n",
    "    sum_accuracy = 0\n",
    "    sum_error = 0\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = generate_titanic_predictions(train, test)\n",
    "        sum_accuracy += compute_accuracy(predictions, [x[3] for x in test])\n",
    "        sum_error += compute_error(predictions, [x[3] for x in test])\n",
    "    accuracy = sum_accuracy / n\n",
    "    error = sum_error / n\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Error Rate:\", error)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a k-NN classifier for the dataset, borrowing from PA4 once more.\n",
    "\n",
    "Since all of the attributes in the titanic dataset are categorical and non-numeric, we will convert them to normalized numeric values instead. Since Age, Sex, and Survived are binary, we will simply arbitrarily assign one value to 0 and one value to 1 for each attribute. For Class, however, there are 4 values: First, Second, Third, and Crew. Here, we make some assumptions about the data in the normalizing process. We can assume that passenger class likely had an impact on survival, since it would affect their living quarter placement as well as potentially their access to lifeboats; presumably, first class passengers would be given priority over crew. Therefore, we will order them with crew being 0 and first class being 1, to hopefully give an accurate normalization of the data.\n",
    "\n",
    "The data will be normalized as follows:\n",
    "\n",
    "| Class  | Normalized |\n",
    "|--------|------------|\n",
    "| crew   |     0      |\n",
    "| third  |   0.333..  |\n",
    "| second |   0.666..  |\n",
    "| first  |     1      |\n",
    "\n",
    "|  Age  | Normalized |\n",
    "|-------|------------|\n",
    "| child |     0      |\n",
    "| adult |     1      |\n",
    "\n",
    "|   Sex  | Normalized |\n",
    "|--------|------------|\n",
    "| female |      0     |\n",
    "|  male  |      1     |\n",
    "\n",
    "| Survived | Normalized |\n",
    "|----------|------------|\n",
    "|    no    |     0      |\n",
    "|   yes    |     1      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_titanic_data(data):\n",
    "    normal_data = []\n",
    "    class_n = {\"crew\":0, \"third\":(1/3), \"second\":(2/3), \"first\":1}\n",
    "    age_n = {\"child\":0, \"adult\":1}\n",
    "    sex_n = {\"female\":0, \"male\":1}\n",
    "    survived_n = {\"no\":0, \"yes\":1}\n",
    "    for instance in data:\n",
    "        instance_n = []\n",
    "        instance_n.append(class_n[instance[0]])\n",
    "        instance_n.append(age_n[instance[1]])\n",
    "        instance_n.append(sex_n[instance[2]])\n",
    "        instance_n.append(survived_n[instance[3]])\n",
    "        normal_data.append(instance_n)\n",
    "    return normal_data\n",
    "\n",
    "titanic_knn = normalize_titanic_data(titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write our k-NN classifier, copying heavily from PA4 but modifying the logic slightly to fit our needs better. We will need `calculate_distance()` to get the distance between two instances, `calculate_all_distances()` for getting a list of distances for the entire dataset, and `calculate_most_common_element()` for k-NN voting, as well as a modified `knn()` function as the actual classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def calculate_distance(instance, base, class_index):\n",
    "    sum_a = 0\n",
    "    for i in range(len(instance)):\n",
    "        if i == class_index:\n",
    "            continue\n",
    "        else:\n",
    "            dif = instance[i] - base[i]\n",
    "            sum_a += dif**2\n",
    "    distance = sum_a**(1/2)\n",
    "    return distance\n",
    "\n",
    "def calculate_all_distances(data, base, class_index):\n",
    "    distances = []\n",
    "    for instance in data:\n",
    "        distance = calculate_distance(instance, base, class_index)\n",
    "        distances.append((instance[class_index], distance))\n",
    "    return distances\n",
    "\n",
    "def compute_most_common_element(data, index):\n",
    "    freqs = {}\n",
    "    for instance in data:\n",
    "        if instance[index] in freqs:\n",
    "            freqs[instance[index]] += 1\n",
    "        else:\n",
    "            freqs[instance[index]] = 1            \n",
    "    # line referenced from https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "    mce = max(freqs.items(), key=operator.itemgetter(1))[0]\n",
    "    return mce\n",
    "\n",
    "def knn(data, instance, k, class_index):\n",
    "    distances = calculate_all_distances(data, instance, class_index)\n",
    "    sorted_distances = sorted(distances, key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(sorted_distances[i][0])\n",
    "    return compute_most_common_element([[x] for x in neighbors], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to define methods to generate multiple k-NN predictions, as well as perform a stratified cross-fold train/test split using k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_knn_predictions(train, test):\n",
    "    predictions = []\n",
    "    for instance in test:\n",
    "        predictions.append(knn(titanic_knn, instance, 5, 3))\n",
    "    return predictions\n",
    "\n",
    "def print_knn_cross_fold_output(data, n):\n",
    "    sum_accuracy = 0\n",
    "    sum_error = 0\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = generate_knn_predictions(train, test)\n",
    "        sum_accuracy += compute_accuracy(predictions, [x[3] for x in test])\n",
    "        sum_error += compute_error(predictions, [x[3] for x in test])\n",
    "    accuracy = sum_accuracy / n\n",
    "    error = sum_error / n\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Error Rate:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our classifiers defined, we can test their output. First, we will run a 10-fold cross validation on the Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7774442538593482\n",
      "Error Rate: 0.22255574614065182\n"
     ]
    }
   ],
   "source": [
    "print_titanic_cross_fold_output(titanic_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then a 10-fold cross validation on the k-NN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7824614065180102\n",
      "Error Rate: 0.2175385934819897\n"
     ]
    }
   ],
   "source": [
    "print_knn_cross_fold_output(titanic_knn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both classifiers produce very similar accuracies, sitting around 78% accurate. To better display these results, we will also create two functions to print confusion matrices for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes (Stratefied10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "Survived      no    yes    Total    Recognition (%)\n",
      "----------  ----  -----  -------  -----------------\n",
      "no           346    362      708            48.8701\n",
      "yes          126   1358     1484            91.5094\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def confusion_matrix_bayes(data, n):\n",
    "    results = [[\"no\", 0, 0], [\"yes\", 0, 0]]\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = generate_titanic_predictions(train, test)\n",
    "        actual = [instance[3] for instance in test]\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == \"no\":\n",
    "                if predictions[i] == \"no\":\n",
    "                    results[0][1] += 1\n",
    "                else:\n",
    "                    results[0][2] += 1\n",
    "            else:\n",
    "                if predictions[i] == \"no\":\n",
    "                    results[1][1] += 1\n",
    "                else:\n",
    "                    results[1][2] += 1\n",
    "    for row in results:\n",
    "        row.append(sum(row[1:]))\n",
    "        if row[0] == \"no\":\n",
    "            row.append((row[1] / row[3]) * 100)\n",
    "        else:\n",
    "            row.append((row[2] / row[3]) * 100)\n",
    "    header = [\"Survived\", \"no\", \"yes\", \"Total\", \"Recognition (%)\"]\n",
    "    print(\"Naive Bayes (Stratefied\", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"===============================================================\")\n",
    "    print(tabulate(results, headers=header))\n",
    "    \n",
    "confusion_matrix_bayes(titanic_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Bayes (Stratefied10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "Survived      no    yes    Total    Recognition (%)\n",
      "----------  ----  -----  -------  -----------------\n",
      "no           357    351      708            50.4237\n",
      "yes          126   1358     1484            91.5094\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix_knn(data, n):\n",
    "    results = [[\"no\", 0, 0], [\"yes\", 0, 0]]\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        predictions = generate_knn_predictions(train, test)\n",
    "        actual = [instance[3] for instance in test]\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == 0:\n",
    "                if predictions[i] == 0:\n",
    "                    results[0][1] += 1\n",
    "                else:\n",
    "                    results[0][2] += 1\n",
    "            else:\n",
    "                if predictions[i] == 0:\n",
    "                    results[1][1] += 1\n",
    "                else:\n",
    "                    results[1][2] += 1\n",
    "    for row in results:\n",
    "        row.append(sum(row[1:]))\n",
    "        if row[0] == \"no\":\n",
    "            row.append((row[1] / row[3]) * 100)\n",
    "        else:\n",
    "            row.append((row[2] / row[3]) * 100)\n",
    "    header = [\"Survived\", \"no\", \"yes\", \"Total\", \"Recognition (%)\"]\n",
    "    print(\"k-NN Bayes (Stratefied\", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"===============================================================\")\n",
    "    print(tabulate(results, headers=header))\n",
    "    \n",
    "confusion_matrix_knn(titanic_knn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Baseline Classifiers\n",
    "\n",
    "To compare the accuracy of the previously created Naive Bayes and k-NN classifiers on the Titanic dataset, we will create two \"baseline\" classifiers to use for reference. First, we will create a Random classifier, which will generate predictions randomly with guesses weighted based on the frequency of class labels in the dataset. Then, we will also create a Zero Rule classifier, which simply always guesses the most common class label in the dataset. The two functions for these classifiers are as follows:\n",
    "\n",
    "* `random_classifier()`\n",
    "    * **Params**:\n",
    "        * `data` - The dataset to use for generating predictions\n",
    "        * `n` - The number of random predictions to generate\n",
    "        * `classIndex` - The index of the classification attribute in the data\n",
    "    * **Returns**:\n",
    "        * A list of `n` random classifications, weighted based on the frequency of class labels in the database (ie if 80% of the data is classified as Yes and 20% as No, the classifier will guess Yes about 80% of the time.)\n",
    "* `zero_r_classifier()`\n",
    "    * **Params**:\n",
    "        * `data` - The dataset to use for generating predictions\n",
    "        * `n` - The number of predictions to generate\n",
    "        * `classIndex` - The index of the classification attribute in the data\n",
    "    * **Returns**:\n",
    "        * A list of `n` instances of the most common class label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_classifier(data, n, classIndex):\n",
    "    classes = []\n",
    "    for instance in data:\n",
    "        classes.append(instance[classIndex])\n",
    "    predictions = []\n",
    "    for i in range(n):\n",
    "        index = randint(0, len(classes))\n",
    "        predictions.append(data[index][classIndex])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def zero_r_classifier(data, n, classIndex):\n",
    "    mce = compute_most_common_element(data, classIndex)\n",
    "    predictions = []\n",
    "    for i in range(n):\n",
    "        predictions.append(mce)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, knowing that our k-NN and Naive Bayes classifiers both have an accuracy of about 78%, let's generate predictions for about 1/3 of our data using our baseline classifiers and compare their accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Classifier Accuracy: 0.5557142857142857\n",
      "Zero Rule Classifier Accuracy: 0.6657142857142857\n"
     ]
    }
   ],
   "source": [
    "test = generate_random_instances(titanic_data, 700)\n",
    "\n",
    "predictions_random = random_classifier(titanic_data, 700, 3)\n",
    "predictions_zero = zero_r_classifier(titanic_data, 700, 3)\n",
    "\n",
    "print(\"Random Classifier Accuracy:\", compute_accuracy(predictions_random, [x[3] for x in test]))\n",
    "print(\"Zero Rule Classifier Accuracy:\", compute_accuracy(predictions_zero, [x[3] for x in test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, these incredibly simple baseline classifiers actually do fairly well for the data, with accuracy between 50-70%. However, our actual classifiers do seem to consistantly be more accurate, although the increase in complexity is much greater than the increase in accuracy for both Naive Bayes and k-NN compared to these simple predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
